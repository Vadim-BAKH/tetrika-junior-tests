"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞, –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏.

–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö,
- –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∏ –ø–æ–¥—Å—á—ë—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ,
- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV —Ñ–∞–π–ª,
- –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å.
"""

import asyncio
from collections import defaultdict
from typing import DefaultDict

import aiofiles
import aiohttp
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://ru.wikipedia.org"
START_URL = BASE_URL + "/w/index.php?title=–ö–∞—Ç–µ–≥–æ—Ä–∏—è:–ñ–∏–≤–æ—Ç–Ω—ã–µ_–ø–æ_–∞–ª—Ñ–∞–≤–∏—Ç—É"


def collect_all_pages() -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É.

    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ HTTP-–∑–∞–ø—Ä–æ—Å—ã –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏,
    –ø–µ—Ä–µ—Ö–æ–¥—è –ø–æ —Å—Å—ã–ª–∫–µ "–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞", –ø–æ–∫–∞ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.

    Returns:
        list[str]: –°–ø–∏—Å–æ–∫ URL –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
    """
    urls = [START_URL]
    url = START_URL
    while True:
        resp = requests.get(url, timeout=20)
        soup = BeautifulSoup(resp.text, "lxml")
        next_link = soup.find("a", string="–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
        if next_link and "href" in next_link.attrs:
            next_url = BASE_URL + next_link["href"]
            urls.append(next_url)
            url = next_url
        else:
            break
    print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(urls)}\n")
    return urls


async def fetch(
        session: aiohttp.ClientSession, url: str
) -> DefaultDict[str, int]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∂–∏–≤–æ—Ç–Ω—ã–º–∏.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ.

    Args:
        session (aiohttp.ClientSession): –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è HTTP.
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è.

    Returns:
        collections.defaultdict[int]: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
        –∂–∏–≤–æ—Ç–Ω—ã—Ö –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ.
    """
    async with session.get(url) as response:
        html = await response.text()
        soup = BeautifulSoup(html, "lxml")
        letters = defaultdict(int)
        for li in soup.select(".mw-category li"):
            name = li.get_text().strip()
            if name:
                first_letter = name[0].upper()
                letters[first_letter] += 1
        return letters


async def parse_all(urls: list[str]) -> DefaultDict[str, int]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL, —Å–æ–±–∏—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º.

    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ fetch –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL –∏ —Å—É–º–º–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

    Args:
        urls (list[str]): –°–ø–∏—Å–æ–∫ URL —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

    Returns:
        collections.defaultdict[int]: –°–ª–æ–≤–∞—Ä—å —Å —Å—É–º–º–∞—Ä–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
        –∂–∏–≤–æ—Ç–Ω—ã—Ö –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ.
    """
    result = defaultdict(int)
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        pages_data = await asyncio.gather(*tasks)
        for page in pages_data:
            for letter, count in page.items():
                result[letter] += count
    return result


async def save_to_csv_async(
        data: dict[str, int], filename: str = "beasts.csv"
) -> None:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–∞–π–ª.

    –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–∞—Ä—ã "–±—É–∫–≤–∞,–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ" –ø–æ—Å—Ç—Ä–æ—á–Ω–æ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ–∞–π–ª.

    Args:
        data (dict[str, int]): –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
        filename (str, optional): –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏.
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é "beasts.csv".
    """
    async with aiofiles.open(
            filename, mode="w", encoding="utf-8", newline=""
    ) as f:
        for letter in sorted(data.keys()):
            line = f"{letter},{data[letter]}\n"
            await f.write(line)
    print(f"\nüìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")


def main() -> None:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–±–æ—Ä–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.

    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    """
    urls = collect_all_pages()
    asyncio.run(parse_and_save(urls))


async def parse_and_save(urls: list[str]) -> None:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
     –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª.

    Args:
        urls (list[str]): –°–ø–∏—Å–æ–∫ URL —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    data = await parse_all(urls)
    await save_to_csv_async(data)

    total = sum(data.values())
    print(f"\nüì¶ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∏–≤–æ—Ç–Ω—ã—Ö: {total}\n")


if __name__ == "__main__":
    main()
